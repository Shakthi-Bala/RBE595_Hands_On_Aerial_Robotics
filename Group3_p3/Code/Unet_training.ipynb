{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8Elqrq7Pz6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57f6edb-c706-4a0b-e5c9-7c165830afd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# ┌───────────────────────────────────────────────┐\n",
        "# │                    IMPORTS                    │\n",
        "# └───────────────────────────────────────────────┘\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "import zipfile\n",
        "from PIL import Image, ImageOps\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import v2 as transforms\n",
        "from PIL import Image\n",
        "# Determine Device for the whole session\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment if not using Personal Google Drive for Hosting Training Data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ORUkH1VOP4kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea8f3e5-e87e-46bb-d4de-9350f2db6104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip dataset\n",
        "file_path=\"/content/drive/MyDrive/RBE595/p3/dataset/window_dataset_12000.zip\"\n",
        "directory_to_extract_to = \"/content/dataset\" # Define the extraction directory\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)"
      ],
      "metadata": {
        "id": "2wWYPfAwq4KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/dataset/window_dataset/images/\"\n",
        "mask_path = \"/content/dataset/window_dataset/masks/\"\n",
        "\n",
        "# List files in the directories to pick one example\n",
        "import os\n",
        "image_files = sorted(os.listdir(image_path))\n",
        "print(len(image_files))\n",
        "mask_files = sorted(os.listdir(mask_path))\n"
      ],
      "metadata": {
        "id": "Oeju1wB4GRfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a63d7f4-2ece-4e2d-ea75-5f2876b6ef2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check image and mask\n",
        "\n",
        "# image_path = \"/content/dataset/window_dataset/images/\"\n",
        "# mask_path = \"/content/dataset/window_dataset/masks/\"\n",
        "\n",
        "# # List files in the directories to pick one example\n",
        "# import os\n",
        "# image_files = sorted(os.listdir(image_path))\n",
        "# mask_files = sorted(os.listdir(mask_path))\n",
        "\n",
        "# if image_files and mask_files:\n",
        "#     first_image_file = image_files[2]\n",
        "#     first_mask_file = mask_files[2]\n",
        "# Aerial_P3_Unet_2.ipynb_\n",
        "# [1]\n",
        "# 11s\n",
        "\n",
        "# # ┌───────────────────────────────────────────────┐\n",
        "# # │                    IMPORTS                    │\n",
        "# # └───────────────────────────────────────────────┘\n",
        "# import matplotlib.pyplot as plt\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from torchvision import datasets, transforms\n",
        "# from torchvision.transforms import v2\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.optim import AdamW\n",
        "# import zipfile\n",
        "# from PIL import Image, ImageOps\n",
        "# import albumentations as A\n",
        "# from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# import os\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# from torchvision.transforms import v2 as transforms\n",
        "# from PIL import Image\n",
        "# # Determine Device for the whole session\n",
        "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Using device: cuda\n",
        "\n",
        "# [2]\n",
        "# 26s\n",
        "\n",
        "# Mounted at /content/drive\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# # Unzip dataset\n",
        "# file_path=\"/content/drive/MyDrive/RBE595/p3/dataset/window_dataset_12000.zip\"\n",
        "# directory_to_extract_to = \"/content/dataset\" # Define the extraction directory\n",
        "# with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(directory_to_extract_to)\n",
        "\n",
        "# [3]\n",
        "# 0s\n",
        "\n",
        "# image_path = \"/content/dataset/window_dataset/images/\"\n",
        "# mask_path = \"/content/dataset/window_dataset/masks/\"\n",
        "\n",
        "# # List files in the directories to pick one example\n",
        "# import os\n",
        "# image_files = sorted(os.listdir(image_path))\n",
        "# print(len(image_files))\n",
        "# mask_files = sorted(os.listdir(mask_path))\n",
        "\n",
        "# Next steps:\n",
        "# [ ]\n",
        "# 0s\n",
        "# [ ]\n",
        "\n",
        "# # ┌───────────────────────────────────────────────┐\n",
        "# # │                 HYPERPARAMETERS               │\n",
        "# # └───────────────────────────────────────────────┘\n",
        "# NUM_EPOCHS = 10\n",
        "# BATCH_SIZE = 8 # Reduced batch size to potentially resolve OOM error\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# Total dataset size: 7000\n",
        "# Train size: 5600\n",
        "# Validation size: 700\n",
        "# Test size: 700\n",
        "\n",
        "# ==================================================\n",
        "# DATALOADER INFORMATION\n",
        "# ==================================================\n",
        "\n",
        "# Train loader: 700 batches\n",
        "# Validation loader: 88 batches\n",
        "# Test loader: 88 batches\n",
        "\n",
        "# /tmp/ipython-input-2431407332.py:35: UserWarning: Argument(s) 'max_holes, max_height, max_width, min_holes, min_height, min_width, fill_value' are not valid for transform CoarseDropout\n",
        "#   A.CoarseDropout(\n",
        "\n",
        "\n",
        "# Train batch - Images shape: torch.Size([8, 3, 320, 320]), Masks shape: torch.Size([8, 1, 320, 320])\n",
        "# Val batch - Images shape: torch.Size([8, 3, 320, 320]), Masks shape: torch.Size([8, 1, 320, 320])\n",
        "# Test batch - Images shape: torch.Size([8, 3, 320, 320]), Masks shape: torch.Size([8, 1, 320, 320])\n",
        "\n",
        "# ✓ All DataLoaders created successfully!\n",
        "# ==================================================\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # --- Attention Block (as you provided) ---\n",
        "# class AttentionBlock(nn.Module):\n",
        "#     def __init__(self, F_g, F_l, F_int):\n",
        "#         super(AttentionBlock, self).__init__()\n",
        "#         self.W_g = nn.Sequential(\n",
        "#             nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "#             nn.BatchNorm2d(F_int)\n",
        "#         )\n",
        "#         self.W_x = nn.Sequential(\n",
        "#             nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "#             nn.BatchNorm2d(F_int)\n",
        "#         )\n",
        "#         self.psi = nn.Sequential(\n",
        "#             nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "#             nn.BatchNorm2d(1),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "#     def forward(self, g, x):\n",
        "#         g1 = self.W_g(g)\n",
        "#         x1 = self.W_x(x)\n",
        "#         psi = self.relu(g1 + x1)\n",
        "#         psi = self.psi(psi)\n",
        "#         return x * psi\n",
        "\n",
        "# # --- Double Conv Block (as you provided) ---\n",
        "# def double_conv(in_channels, out_channels):\n",
        "#     '''\n",
        "#     Return a Convolutional Block with Two Convolutional Layers with ReLU Activation based on in_channels and\n",
        "#     out_channels, with a stride of 3 and padding of 1. If input has H and W, then output will have same H and W.\n",
        "#     '''\n",
        "#     return nn.Sequential(\n",
        "#         nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True),\n",
        "#         nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     )\n",
        "\n",
        "# # --- NEW: Attention U-Net Class (Modified) ---\n",
        "# class UNet(nn.Module):\n",
        "#     '''\n",
        "#     Implementation of an Attention U-Net (3-level).\n",
        "#     '''\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # --- Encoder (3 levels) ---\n",
        "#         self.dconv_down1 = double_conv(in_channels, 64)\n",
        "#         self.dconv_down2 = double_conv(64, 128)\n",
        "#         self.dconv_down3 = double_conv(128, 256)\n",
        "#         # Removed dconv_down4\n",
        "#         self.maxpool = nn.MaxPool2d(2)\n",
        "\n",
        "#         # --- Bottleneck ---\n",
        "#         # Input from 3rd level (256 channels), output 512\n",
        "#         self.bottleneck = double_conv(256, 512)\n",
        "\n",
        "#         # --- Decoder ---\n",
        "#         self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "#         # --- Attention Blocks ---\n",
        "#         # Removed att4\n",
        "#         # F_g=512 (from upsample(bottleneck)), F_l=256 (from conv3)\n",
        "#         self.att3 = AttentionBlock(F_g=512, F_l=256, F_int=128)\n",
        "#         # F_g=256 (from dconv_up3), F_l=128 (from conv2)\n",
        "#         self.att2 = AttentionBlock(F_g=256, F_l=128, F_int=64)\n",
        "#         # F_g=128 (from dconv_up2), F_l=64 (from conv1)\n",
        "#         self.att1 = AttentionBlock(F_g=128, F_l=64, F_int=32)\n",
        "\n",
        "#         # --- Decoder Convolutions ---\n",
        "#         # Removed dconv_up4\n",
        "#         # Input: (g=512) + (att3=256) = 768\n",
        "#         self.dconv_up3 = double_conv(512 + 256, 256)  # 768  -> 256\n",
        "#         # Input: (g=256) + (att2=128) = 384\n",
        "#         self.dconv_up2 = double_conv(256 + 128, 128)  # 384  -> 128\n",
        "#         # Input: (g=128) + (att1=64) = 192\n",
        "#         self.dconv_up1 = double_conv(128 + 64, 64)   # 192  -> 64\n",
        "\n",
        "#         # --- Output Layer ---\n",
        "#         self.conv_last = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         '''\n",
        "#         Forward Pass Method for the Attention UNet\n",
        "#         '''\n",
        "\n",
        "#         # ┌───────────────────────────────────────────────┐\n",
        "#         # │         ENCODER BLOCKS (DOWNSAMPLING)         │\n",
        "#         # └───────────────────────────────────────────────┘\n",
        "#         conv1 = self.dconv_down1(x)                                 # (N, 64, H, W)\n",
        "#         x = self.maxpool(conv1)\n",
        "\n",
        "#         conv2 = self.dconv_down2(x)                                 # (N, 128, H/2, W/2)\n",
        "#         x = self.maxpool(conv2)\n",
        "\n",
        "#         conv3 = self.dconv_down3(x)                                 # (N, 256, H/4, W/4)\n",
        "#         x = self.maxpool(conv3)                                     # (N, 256, H/8, W/8)\n",
        "\n",
        "#         # Removed 4th Encoder Block\n",
        "\n",
        "#         # ┌───────────────────────────────────────────────┐\n",
        "#         # │         BOTTLENECK                            │\n",
        "#         # └───────────────────────────────────────────────┘\n",
        "#         x = self.bottleneck(x)                                      # (N, 512, H/8, W/8)\n",
        "\n",
        "#         # ┌───────────────────────────────────────────────┐\n",
        "#         # │           DECODER BLOCKS (UPSAMPLING)         │\n",
        "#         # └───────────────────────────────────────────────┘\n",
        "\n",
        "#         # --- First Decoder Block (was Second) ---\n",
        "#         g = self.upsample(x)                                        # (N, 512, H/4, W/4)\n",
        "#         x_skip3 = conv3                                             # (N, 256, H/4, W/4)\n",
        "#         att3 = self.att3(g=g, x=x_skip3)                            # (N, 256, H/4, W/4)\n",
        "#         x = torch.cat([g, att3], dim=1)                             # (N, 512 + 256, H/4, W/4)\n",
        "#         x = self.dconv_up3(x)                                       # (N, 256, H/4, W/4)\n",
        "\n",
        "#         # --- Second Decoder Block (was Third) ---\n",
        "#         g = self.upsample(x)                                        # (N, 256, H/2, W/2)\n",
        "#         x_skip2 = conv2                                             # (N, 128, H/2, W/2)\n",
        "#         att2 = self.att2(g=g, x=x_skip2)                            # (N, 128, H/2, W/2)\n",
        "#         x = torch.cat([g, att2], dim=1)                             # (N, 256 + 128, H/2, W/2)\n",
        "#         x = self.dconv_up2(x)                                       # (N, 128, H/2, W/2)\n",
        "\n",
        "#         # --- Third Decoder Block (was Fourth) ---\n",
        "#         g = self.upsample(x)                                        # (N, 128, H, W)\n",
        "#         x_skip1 = conv1                                             # (N, 64, H, W)\n",
        "#         att1 = self.att1(g=g, x=x_skip1)                            # (N, 64, H, W)\n",
        "#         x = torch.cat([g, att1], dim=1)                             # (N, 128 + 64, H, W)\n",
        "#         x = self.dconv_up1(x)                                       # (N, 64, H, W)\n",
        "\n",
        "#         # --- Final Output Layer ---\n",
        "#         out = self.conv_last(x)                                     # (N, out_channels, H, W)\n",
        "\n",
        "#         return out\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# class CombinedLoss(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Combination of BCE, Dice, and boundary loss for clean edges\n",
        "#     \"\"\"\n",
        "#     def __init__(self, bce_weight=0.5, dice_weight=0.3, boundary_weight=0.2):\n",
        "#         super().__init__()\n",
        "#         self.bce_weight = bce_weight\n",
        "#         self.dice_weight = dice_weight\n",
        "#         self.boundary_weight = boundary_weight\n",
        "#         self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#     def dice_loss(self, pred, target, smooth=1e-6):\n",
        "#         pred = torch.sigmoid(pred)\n",
        "#         intersection = (pred * target).sum(dim=(2, 3))\n",
        "#         union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
        "#         dice = (2. * intersection + smooth) / (union + smooth)\n",
        "#         return 1 - dice.mean()\n",
        "\n",
        "#     def boundary_loss(self, pred, target):\n",
        "#         \"\"\"\n",
        "#         Penalize predictions at boundaries more heavily\n",
        "#         \"\"\"\n",
        "#         pred = torch.sigmoid(pred)\n",
        "\n",
        "#         # Compute gradient of target (boundaries)\n",
        "#         kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
        "#                                 dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n",
        "#         kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
        "#                                 dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n",
        "\n",
        "#         # Detect edges in target\n",
        "#         edge_x = F.conv2d(target, kernel_x, padding=1)\n",
        "#         edge_y = F.conv2d(target, kernel_y, padding=1)\n",
        "#         edges = torch.sqrt(edge_x**2 + edge_y**2)\n",
        "#         edges = (edges > 0.5).float()\n",
        "\n",
        "#         # Weight loss by edges (penalize errors at boundaries more)\n",
        "#         boundary_error = F.binary_cross_entropy(pred, target, reduction='none')\n",
        "#         boundary_error = boundary_error * (1 + 5 * edges)  # 5x penalty at edges\n",
        "\n",
        "#         return boundary_error.mean()\n",
        "\n",
        "#     def forward(self, pred, target):\n",
        "#         bce = self.bce(pred, target)\n",
        "#         dice = self.dice_loss(pred, target)\n",
        "#         boundary = self.boundary_loss(pred, target)\n",
        "\n",
        "#         total = (self.bce_weight * bce +\n",
        "#                 self.dice_weight * dice +\n",
        "#                 self.boundary_weight * boundary)\n",
        "#         return total\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# # Model initialisation\n",
        "\n",
        "# model= UNet(in_channels=3, out_channels=1).to(DEVICE)\n",
        "# BCE_loss = nn.BCEWithLogitsLoss()\n",
        "# criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.3, boundary_weight=0.2)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "# def calculate_dice_iou(inputs, targets, smooth=1e-7):\n",
        "#     \"\"\"\n",
        "#     Calculates Dice and IoU scores for a batch.\n",
        "#     Assumes inputs are binary predictions (0.0 or 1.0) and targets are binary masks.\n",
        "#     \"\"\"\n",
        "#     # Flatten tensors\n",
        "#     inputs = inputs.view(-1)\n",
        "#     targets = targets.view(-1)\n",
        "\n",
        "#     # Calculate intersection\n",
        "#     intersection = (inputs * targets).sum()\n",
        "\n",
        "#     # Calculate Dice Score\n",
        "#     total_sum = inputs.sum() + targets.sum()\n",
        "#     dice = (2. * intersection + smooth) / (total_sum + smooth)\n",
        "\n",
        "#     # Calculate IoU (Jaccard Index)\n",
        "#     union = total_sum - intersection\n",
        "#     iou = (intersection + smooth) / (union + smooth)\n",
        "\n",
        "#     return dice.item(), iou.item()\n",
        "\n",
        "# [ ]\n",
        "\n",
        "\n",
        "# # Training\n",
        "# best_val_dice = -1  # Initialize best_val_dice to a low value\n",
        "\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#   train_loss=0\n",
        "\n",
        "#   model.train()\n",
        "#   for images, mask in train_loader:\n",
        "#     images=images.to(DEVICE)\n",
        "#     mask=mask.to(DEVICE)\n",
        "#     optimizer.zero_grad()\n",
        "#     output=model(images)\n",
        "#     # bce_loss=BCE_loss(output, mask)\n",
        "#     # dice_loss_value=dice_loss(output, mask)\n",
        "#     # loss=0.5*bce_loss+0.5*dice_loss_value\n",
        "#     loss=criterion(output, mask)\n",
        "\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     train_loss+=loss.item()\n",
        "\n",
        "#   avg_train_loss = train_loss / len(train_loader)\n",
        "# # validation\n",
        "#   model.eval()\n",
        "#   tatal_loss=0\n",
        "#   total_dice_loss=0\n",
        "#   total_bce_loss=0\n",
        "#   total_dice_score=0\n",
        "#   total_iou_score=0\n",
        "#   total_val_loss=0\n",
        "#   with torch.no_grad():\n",
        "#     for images, mask in val_loader:\n",
        "\n",
        "#       images=images.to(DEVICE)\n",
        "#       mask=mask.to(DEVICE)\n",
        "#       output=model(images)\n",
        "#       # bce = BCE_loss(output, mask)\n",
        "#       # dice_val_loss = dice_loss(output, mask)\n",
        "#       # loss = 0.5 * bce + 0.5 * dice_val_loss\n",
        "#       loss=criterion(output, mask)\n",
        "#       total_val_loss += loss.item()\n",
        "#       probs=torch.sigmoid(output)\n",
        "#       binary_preds = (probs > 0.5).float()\n",
        "#       dice_score, iou_score = calculate_dice_iou(binary_preds, mask)\n",
        "#       total_dice_score += dice_score\n",
        "#       total_iou_score += iou_score\n",
        "\n",
        "#     # Calculate average metrics for the epoch\n",
        "#     avg_val_loss = total_val_loss / len(val_loader)\n",
        "#     avg_val_dice = total_dice_score / len(val_loader)\n",
        "#     avg_val_iou = total_iou_score / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{NUM_EPOCHS}.. \"\n",
        "#           f\"Train Loss: {avg_train_loss:.4f}.. \"\n",
        "#           f\"Val Loss: {avg_val_loss:.4f}.. \"\n",
        "#           f\"Val Dice: {avg_val_dice:.4f}.. \"\n",
        "#           f\"Val IoU: {avg_val_iou:.4f}\")\n",
        "\n",
        "#     # --- Save the best model ---\n",
        "#     if 'best_val_dice' in locals() and avg_val_dice > best_val_dice:\n",
        "#         best_val_dice = avg_val_dice\n",
        "#         torch.save(model.state_dict(), 'UNet_background_attention_1.pth')\n",
        "#         print(f\"New best model saved with Dice: {avg_val_dice:.4f}\")\n",
        "\n",
        "# Epoch 1/10.. Train Loss: 0.1810.. Val Loss: 0.0342.. Val Dice: 0.9704.. Val IoU: 0.9427\n",
        "# New best model saved with Dice: 0.9704\n",
        "# Epoch 2/10.. Train Loss: 0.1020.. Val Loss: 0.0277.. Val Dice: 0.9736.. Val IoU: 0.9487\n",
        "# New best model saved with Dice: 0.9736\n",
        "# Epoch 3/10.. Train Loss: 0.0860.. Val Loss: 0.0306.. Val Dice: 0.9674.. Val IoU: 0.9372\n",
        "# Epoch 4/10.. Train Loss: 0.0833.. Val Loss: 0.0214.. Val Dice: 0.9782.. Val IoU: 0.9574\n",
        "# New best model saved with Dice: 0.9782\n",
        "# Epoch 5/10.. Train Loss: 0.0747.. Val Loss: 0.0210.. Val Dice: 0.9785.. Val IoU: 0.9580\n",
        "# New best model saved with Dice: 0.9785\n",
        "# Epoch 6/10.. Train Loss: 0.0692.. Val Loss: 0.0162.. Val Dice: 0.9831.. Val IoU: 0.9668\n",
        "# New best model saved with Dice: 0.9831\n",
        "# Epoch 7/10.. Train Loss: 0.0637.. Val Loss: 0.0181.. Val Dice: 0.9805.. Val IoU: 0.9618\n",
        "# Epoch 8/10.. Train Loss: 0.0586.. Val Loss: 0.0169.. Val Dice: 0.9818.. Val IoU: 0.9643\n",
        "# Epoch 9/10.. Train Loss: 0.0553.. Val Loss: 0.0233.. Val Dice: 0.9718.. Val IoU: 0.9454\n",
        "# Epoch 10/10.. Train Loss: 0.0552.. Val Loss: 0.0166.. Val Dice: 0.9829.. Val IoU: 0.9664\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# # --- Testing ---\n",
        "# print(\"\\nTraining Finished. Loading best model for testing.\")\n",
        "# model.load_state_dict(torch.load('UNet_background_attention_1.pth')) # Load best weights\n",
        "# model.eval()\n",
        "\n",
        "# total_test_dice = 0\n",
        "# total_test_iou = 0\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for images, mask in test_loader:\n",
        "#         images = images.to(DEVICE)\n",
        "#         mask = mask.to(DEVICE)\n",
        "#         output = model(images)\n",
        "\n",
        "#         probs = torch.sigmoid(output)\n",
        "#         binary_preds = (probs > 0.5).float()\n",
        "\n",
        "#         dice_score, iou_score = calculate_dice_iou(binary_preds, mask)\n",
        "#         total_test_dice += dice_score\n",
        "#         total_test_iou += iou_score\n",
        "\n",
        "# avg_test_dice = total_test_dice / len(test_loader)\n",
        "# avg_test_iou = total_test_iou / len(test_loader)\n",
        "\n",
        "# print(f\"--- Final Test Metrics ---\")\n",
        "# print(f\"Test Dice Score: {avg_test_dice:.4f}\")\n",
        "# print(f\"Test IoU Score:  {avg_test_iou:.4f}\")\n",
        "\n",
        "\n",
        "# Training Finished. Loading best model for testing.\n",
        "# --- Final Test Metrics ---\n",
        "# Test Dice Score: 0.9829\n",
        "# Test IoU Score:  0.9665\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# import torch\n",
        "# import os\n",
        "# import torchvision.utils as vutils # Import for saving image grids\n",
        "\n",
        "# # --- Testing ---\n",
        "# print(\"\\nTraining Finished. Loading best model for testing.\")\n",
        "# model.load_state_dict(torch.load('UNet_background_attention_1.pth')) # Load best weights\n",
        "# model.eval()\n",
        "\n",
        "# total_test_dice = 0\n",
        "# total_test_iou = 0\n",
        "\n",
        "# # --- Additions for saving output ---\n",
        "# output_dir = \"test_outputs\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "# print(f\"Saving output masks to {output_dir}/\")\n",
        "# # -------------------------------------\n",
        "\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     # Add 'i' for enumeration to get unique filenames\n",
        "#     for i, (images, mask) in enumerate(test_loader):\n",
        "#         images = images.to(DEVICE)\n",
        "#         mask = mask.to(DEVICE)\n",
        "#         output = model(images)\n",
        "\n",
        "#         probs = torch.sigmoid(output)\n",
        "#         binary_preds = (probs > 0.5).float()\n",
        "\n",
        "#         dice_score, iou_score = calculate_dice_iou(binary_preds, mask)\n",
        "#         total_test_dice += dice_score\n",
        "#         total_test_iou += iou_score\n",
        "\n",
        "#         # --- Save the output masks ---\n",
        "#         # This saves the input, ground truth, and prediction for this batch\n",
        "#         # 'vutils.save_image' automatically creates a grid if your batch size > 1\n",
        "\n",
        "#         # Save the original input images\n",
        "#         vutils.save_image(images,\n",
        "#                           os.path.join(output_dir, f\"batch_{i:03d}_input.png\"),\n",
        "#                           normalize=True) # Normalize for proper viewing\n",
        "\n",
        "#         # Save the ground truth masks\n",
        "#         vutils.save_image(mask,\n",
        "#                           os.path.join(output_dir, f\"batch_{i:03d}_ground_truth.png\"))\n",
        "\n",
        "#         # Save your model's predictions\n",
        "#         vutils.save_image(binary_preds,\n",
        "#                           os.path.join(output_dir, f\"batch_{i:03d}_prediction.png\"))\n",
        "#         # -----------------------------\n",
        "\n",
        "# avg_test_dice = total_test_dice / len(test_loader)\n",
        "# avg_test_iou = total_test_iou / len(test_loader)\n",
        "\n",
        "# print(f\"--- Final Test Metrics ---\")\n",
        "# print(f\"Test Dice Score: {avg_test_dice:.4f}\")\n",
        "# print(f\"Test IoU Score:  {avg_test_iou:.4f}\")\n",
        "\n",
        "# print(f\"\\nFinished testing. All output images saved to '{output_dir}'.\")\n",
        "\n",
        "\n",
        "# Training Finished. Loading best model for testing.\n",
        "# Saving output masks to test_outputs/\n",
        "# --- Final Test Metrics ---\n",
        "# Test Dice Score: 0.9829\n",
        "# Test IoU Score:  0.9665\n",
        "\n",
        "# Finished testing. All output images saved to 'test_outputs'.\n",
        "\n",
        "# [ ]\n",
        "\n",
        "\n",
        "# Start coding or generate with AI.\n",
        "\n",
        "# Colab paid products - Cancel contracts here\n",
        "\n",
        "\n",
        "#     image = plt.imread(os.path.join(image_path, first_image_file))\n",
        "#     mask = plt.imread(os.path.join(mask_path, first_mask_file))\n",
        "\n",
        "#     plt.imshow(image)\n",
        "#     plt.title(\"Example Image\")\n",
        "#     plt.show()\n",
        "\n",
        "#     plt.imshow(mask, cmap='gray') # Use gray colormap for masks\n",
        "#     plt.title(\"Example Mask\")\n",
        "#     plt.show()\n",
        "# else:\n",
        "#     print(\"No image or mask files found in the specified directories.\")"
      ],
      "metadata": {
        "id": "X0c06fxdtIvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmcKik8H9R32"
      },
      "outputs": [],
      "source": [
        "# ┌───────────────────────────────────────────────┐\n",
        "# │                 HYPERPARAMETERS               │\n",
        "# └───────────────────────────────────────────────┘\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 4 # Reduced batch size to potentially resolve OOM error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeqhuMhp3VlM",
        "outputId": "383e4846-63a1-4ea3-c755-1157790854d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset size: 12000\n",
            "Train size: 9600\n",
            "Validation size: 1200\n",
            "Test size: 1200\n",
            "\n",
            "==================================================\n",
            "DATALOADER INFORMATION\n",
            "==================================================\n",
            "\n",
            "Train loader: 2400 batches\n",
            "Validation loader: 300 batches\n",
            "Test loader: 300 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2475501229.py:35: UserWarning: Argument(s) 'max_holes, max_height, max_width, min_holes, min_height, min_width, fill_value' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train batch - Images shape: torch.Size([4, 3, 640, 640]), Masks shape: torch.Size([4, 1, 640, 640])\n",
            "Val batch - Images shape: torch.Size([4, 3, 640, 640]), Masks shape: torch.Size([4, 1, 640, 640])\n",
            "Test batch - Images shape: torch.Size([4, 3, 640, 640]), Masks shape: torch.Size([4, 1, 640, 640])\n",
            "\n",
            "✓ All DataLoaders created successfully!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    '''\n",
        "    This dataset loads an image and its corresponding segmentation mask\n",
        "    using Albumentations for robust augmentation.\n",
        "    '''\n",
        "    def __init__(self, image_dir, mask_dir, is_train=True):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "\n",
        "        self.image_files = sorted(os.listdir(self.image_dir))\n",
        "        self.mask_files = sorted(os.listdir(self.mask_dir))\n",
        "\n",
        "        if is_train:\n",
        "            # 1. Training transforms (Merged List)\n",
        "            self.transform = A.Compose([\n",
        "                # --- Essential (Keep at top) ---\n",
        "                A.Resize(height=640, width=640, p=1.0),\n",
        "\n",
        "                # --- Spatial Augmentations ---\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),      # Updated p=0.5\n",
        "                A.Rotate(limit=45, p=0.5),    # ADDED\n",
        "\n",
        "                # --- Color Augmentations ---\n",
        "                A.ToGray(p=0.1), # Kept from previous version\n",
        "                A.RandomBrightnessContrast(p=0.3), # Updated\n",
        "\n",
        "                # --- Noise, Blur, and Edge Augmentations ---\n",
        "                A.GaussNoise(p=0.4),               # Updated\n",
        "                A.GaussianBlur(blur_limit=(3, 7), p=0.4), # ADDED\n",
        "                A.MotionBlur(blur_limit=7, p=0.3),      # ADDED\n",
        "                A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.4), # ADDED\n",
        "\n",
        "                # --- Dropout Augmentation ---\n",
        "                A.CoarseDropout(\n",
        "                    max_holes=8, max_height=20, max_width=20, # Updated\n",
        "                    min_holes=3, min_height=5, min_width=5,\n",
        "                    fill_value=0, p=0.4\n",
        "                ),\n",
        "\n",
        "                # --- Final conversion (Keep at bottom) ---\n",
        "                A.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "                ToTensorV2() # Converts image and mask to PyTorch tensors\n",
        "            ])\n",
        "        else:\n",
        "            # 2. Validation/Test transforms (Unchanged)\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(height=640, width=640, p=1.0),\n",
        "                A.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        mask_name = self.mask_files[idx]\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = ImageOps.exif_transpose(image) # Auto-orientation\n",
        "\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        image_np = np.array(image)\n",
        "        mask_np = np.array(mask)\n",
        "\n",
        "        # Ensure mask is binary (0 or 1)\n",
        "        mask_np = (mask_np > 128).astype(np.uint8) * 255 # Assuming mask is grayscale with values > 128 as foreground\n",
        "\n",
        "        transformed = self.transform(image=image_np, mask=mask_np)\n",
        "\n",
        "        image_tensor = transformed['image']\n",
        "        mask_tensor = transformed['mask'].unsqueeze(0).float() / 255.0 # Normalize mask to 0-1\n",
        "\n",
        "        return image_tensor, mask_tensor\n",
        "\n",
        "# --- Create Train, Validation, and Test DataLoaders ---\n",
        "\n",
        "# Define your paths and parameters\n",
        "image_path = \"/content/dataset/window_dataset/images/\"\n",
        "mask_path = \"/content/dataset/window_dataset/masks/\"\n",
        "\n",
        "\n",
        "# Define split ratios\n",
        "TRAIN_RATIO = 0.8  # 80% for training\n",
        "VAL_RATIO = 0.1   # 10% for validation\n",
        "TEST_RATIO = 0.1  # 10% for testing\n",
        "\n",
        "# Create the full dataset (with training augmentation for now)\n",
        "full_dataset = CustomDataset(image_path, mask_path, is_train=True)\n",
        "\n",
        "# Calculate split sizes\n",
        "dataset_size = len(full_dataset)\n",
        "train_size = int(TRAIN_RATIO * dataset_size)\n",
        "val_size = int(VAL_RATIO * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size  # Remaining goes to test\n",
        "\n",
        "print(f\"Total dataset size: {dataset_size}\")\n",
        "print(f\"Train size: {train_size}\")\n",
        "print(f\"Validation size: {val_size}\")\n",
        "print(f\"Test size: {test_size}\")\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
        ")\n",
        "\n",
        "# Update the is_train flag for validation and test datasets\n",
        "# Create new dataset instances for val and test without augmentation\n",
        "val_dataset_no_aug = CustomDataset(image_path, mask_path, is_train=False)\n",
        "test_dataset_no_aug = CustomDataset(image_path, mask_path, is_train=False)\n",
        "\n",
        "# Get the same indices from the split\n",
        "val_indices = val_dataset.indices\n",
        "test_indices = test_dataset.indices\n",
        "\n",
        "# Create subsets with no augmentation\n",
        "from torch.utils.data import Subset\n",
        "val_dataset = Subset(val_dataset_no_aug, val_indices)\n",
        "test_dataset = Subset(test_dataset_no_aug, test_indices)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffle training data\n",
        "    num_workers=2,  # Parallel data loading\n",
        "    pin_memory=True  # Faster data transfer to GPU\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # Don't shuffle validation data\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # Don't shuffle test data\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Print DataLoader information\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATALOADER INFORMATION\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nTrain loader: {len(train_loader)} batches\")\n",
        "print(f\"Validation loader: {len(val_loader)} batches\")\n",
        "print(f\"Test loader: {len(test_loader)} batches\")\n",
        "\n",
        "# Verify shapes\n",
        "try:\n",
        "    train_images, train_masks = next(iter(train_loader))\n",
        "    print(f\"\\nTrain batch - Images shape: {train_images.shape}, Masks shape: {train_masks.shape}\")\n",
        "\n",
        "    val_images, val_masks = next(iter(val_loader))\n",
        "    print(f\"Val batch - Images shape: {val_images.shape}, Masks shape: {val_masks.shape}\")\n",
        "\n",
        "    test_images, test_masks = next(iter(test_loader))\n",
        "    print(f\"Test batch - Images shape: {test_images.shape}, Masks shape: {test_masks.shape}\")\n",
        "\n",
        "    print(\"\\n✓ All DataLoaders created successfully!\")\n",
        "    print(\"=\"*50)\n",
        "except Exception as e:\n",
        "    print(f\"\\nError loading batches: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Attention Block (as you provided) ---\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "\n",
        "# --- Double Conv Block (as you provided) ---\n",
        "def double_conv(in_channels, out_channels):\n",
        "    '''\n",
        "    Return a Convolutional Block with Two Convolutional Layers with ReLU Activation based on in_channels and\n",
        "    out_channels, with a stride of 3 and padding of 1. If input has H and W, then output will have same H and W.\n",
        "    '''\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "# --- NEW: Attention U-Net Class (Modified) ---\n",
        "class UNet(nn.Module):\n",
        "    '''\n",
        "    Implementation of an Attention U-Net (3-level).\n",
        "    '''\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Encoder (3 levels) ---\n",
        "        self.dconv_down1 = double_conv(in_channels, 64)\n",
        "        self.dconv_down2 = double_conv(64, 128)\n",
        "        self.dconv_down3 = double_conv(128, 256)\n",
        "        # Removed dconv_down4\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "\n",
        "        # --- Bottleneck ---\n",
        "        # Input from 3rd level (256 channels), output 512\n",
        "        self.bottleneck = double_conv(256, 512)\n",
        "\n",
        "        # --- Decoder ---\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # --- Attention Blocks ---\n",
        "        # Removed att4\n",
        "        # F_g=512 (from upsample(bottleneck)), F_l=256 (from conv3)\n",
        "        self.att3 = AttentionBlock(F_g=512, F_l=256, F_int=128)\n",
        "        # F_g=256 (from dconv_up3), F_l=128 (from conv2)\n",
        "        self.att2 = AttentionBlock(F_g=256, F_l=128, F_int=64)\n",
        "        # F_g=128 (from dconv_up2), F_l=64 (from conv1)\n",
        "        self.att1 = AttentionBlock(F_g=128, F_l=64, F_int=32)\n",
        "\n",
        "        # --- Decoder Convolutions ---\n",
        "        # Removed dconv_up4\n",
        "        # Input: (g=512) + (att3=256) = 768\n",
        "        self.dconv_up3 = double_conv(512 + 256, 256)  # 768  -> 256\n",
        "        # Input: (g=256) + (att2=128) = 384\n",
        "        self.dconv_up2 = double_conv(256 + 128, 128)  # 384  -> 128\n",
        "        # Input: (g=128) + (att1=64) = 192\n",
        "        self.dconv_up1 = double_conv(128 + 64, 64)   # 192  -> 64\n",
        "\n",
        "        # --- Output Layer ---\n",
        "        self.conv_last = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Forward Pass Method for the Attention UNet\n",
        "        '''\n",
        "\n",
        "        # ┌───────────────────────────────────────────────┐\n",
        "        # │         ENCODER BLOCKS (DOWNSAMPLING)         │\n",
        "        # └───────────────────────────────────────────────┘\n",
        "        conv1 = self.dconv_down1(x)                                 # (N, 64, H, W)\n",
        "        x = self.maxpool(conv1)\n",
        "\n",
        "        conv2 = self.dconv_down2(x)                                 # (N, 128, H/2, W/2)\n",
        "        x = self.maxpool(conv2)\n",
        "\n",
        "        conv3 = self.dconv_down3(x)                                 # (N, 256, H/4, W/4)\n",
        "        x = self.maxpool(conv3)                                     # (N, 256, H/8, W/8)\n",
        "\n",
        "        # Removed 4th Encoder Block\n",
        "\n",
        "        # ┌───────────────────────────────────────────────┐\n",
        "        # │         BOTTLENECK                            │\n",
        "        # └───────────────────────────────────────────────┘\n",
        "        x = self.bottleneck(x)                                      # (N, 512, H/8, W/8)\n",
        "\n",
        "        # ┌───────────────────────────────────────────────┐\n",
        "        # │           DECODER BLOCKS (UPSAMPLING)         │\n",
        "        # └───────────────────────────────────────────────┘\n",
        "\n",
        "        # --- First Decoder Block (was Second) ---\n",
        "        g = self.upsample(x)                                        # (N, 512, H/4, W/4)\n",
        "        x_skip3 = conv3                                             # (N, 256, H/4, W/4)\n",
        "        att3 = self.att3(g=g, x=x_skip3)                            # (N, 256, H/4, W/4)\n",
        "        x = torch.cat([g, att3], dim=1)                             # (N, 512 + 256, H/4, W/4)\n",
        "        x = self.dconv_up3(x)                                       # (N, 256, H/4, W/4)\n",
        "\n",
        "        # --- Second Decoder Block (was Third) ---\n",
        "        g = self.upsample(x)                                        # (N, 256, H/2, W/2)\n",
        "        x_skip2 = conv2                                             # (N, 128, H/2, W/2)\n",
        "        att2 = self.att2(g=g, x=x_skip2)                            # (N, 128, H/2, W/2)\n",
        "        x = torch.cat([g, att2], dim=1)                             # (N, 256 + 128, H/2, W/2)\n",
        "        x = self.dconv_up2(x)                                       # (N, 128, H/2, W/2)\n",
        "\n",
        "        # --- Third Decoder Block (was Fourth) ---\n",
        "        g = self.upsample(x)                                        # (N, 128, H, W)\n",
        "        x_skip1 = conv1                                             # (N, 64, H, W)\n",
        "        att1 = self.att1(g=g, x=x_skip1)                            # (N, 64, H, W)\n",
        "        x = torch.cat([g, att1], dim=1)                             # (N, 128 + 64, H, W)\n",
        "        x = self.dconv_up1(x)                                       # (N, 64, H, W)\n",
        "\n",
        "        # --- Final Output Layer ---\n",
        "        out = self.conv_last(x)                                     # (N, out_channels, H, W)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "9AMkGiSKRCHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combination of BCE, Dice, and boundary loss for clean edges\n",
        "    \"\"\"\n",
        "    def __init__(self, bce_weight=0.5, dice_weight=0.3, boundary_weight=0.2):\n",
        "        super().__init__()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "        self.boundary_weight = boundary_weight\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def dice_loss(self, pred, target, smooth=1e-6):\n",
        "        pred = torch.sigmoid(pred)\n",
        "        intersection = (pred * target).sum(dim=(2, 3))\n",
        "        union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
        "        dice = (2. * intersection + smooth) / (union + smooth)\n",
        "        return 1 - dice.mean()\n",
        "\n",
        "    def boundary_loss(self, pred, target):\n",
        "        \"\"\"\n",
        "        Penalize predictions at boundaries more heavily\n",
        "        \"\"\"\n",
        "        pred = torch.sigmoid(pred)\n",
        "\n",
        "        # Compute gradient of target (boundaries)\n",
        "        kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
        "                                dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n",
        "        kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
        "                                dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n",
        "\n",
        "        # Detect edges in target\n",
        "        edge_x = F.conv2d(target, kernel_x, padding=1)\n",
        "        edge_y = F.conv2d(target, kernel_y, padding=1)\n",
        "        edges = torch.sqrt(edge_x**2 + edge_y**2)\n",
        "        edges = (edges > 0.5).float()\n",
        "\n",
        "        # Weight loss by edges (penalize errors at boundaries more)\n",
        "        boundary_error = F.binary_cross_entropy(pred, target, reduction='none')\n",
        "        boundary_error = boundary_error * (1 + 5 * edges)  # 5x penalty at edges\n",
        "\n",
        "        return boundary_error.mean()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        bce = self.bce(pred, target)\n",
        "        dice = self.dice_loss(pred, target)\n",
        "        boundary = self.boundary_loss(pred, target)\n",
        "\n",
        "        total = (self.bce_weight * bce +\n",
        "                self.dice_weight * dice +\n",
        "                self.boundary_weight * boundary)\n",
        "        return total"
      ],
      "metadata": {
        "id": "FA10wS08lzdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model initialisation\n",
        "\n",
        "model= UNet(in_channels=3, out_channels=1).to(DEVICE)\n",
        "BCE_loss = nn.BCEWithLogitsLoss()\n",
        "criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.3, boundary_weight=0.2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "1WewPNbMinRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "def calculate_dice_iou(inputs, targets, smooth=1e-7):\n",
        "    \"\"\"\n",
        "    Calculates Dice and IoU scores for a batch.\n",
        "    Assumes inputs are binary predictions (0.0 or 1.0) and targets are binary masks.\n",
        "    \"\"\"\n",
        "    # Flatten tensors\n",
        "    inputs = inputs.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    # Calculate intersection\n",
        "    intersection = (inputs * targets).sum()\n",
        "\n",
        "    # Calculate Dice Score\n",
        "    total_sum = inputs.sum() + targets.sum()\n",
        "    dice = (2. * intersection + smooth) / (total_sum + smooth)\n",
        "\n",
        "    # Calculate IoU (Jaccard Index)\n",
        "    union = total_sum - intersection\n",
        "    iou = (intersection + smooth) / (union + smooth)\n",
        "\n",
        "    return dice.item(), iou.item()\n"
      ],
      "metadata": {
        "id": "m0E0LnOxJKNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training\n",
        "best_val_dice = -1  # Initialize best_val_dice to a low value\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_loss=0\n",
        "\n",
        "  model.train()\n",
        "  for images, mask in train_loader:\n",
        "    images=images.to(DEVICE)\n",
        "    mask=mask.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    output=model(images)\n",
        "    # bce_loss=BCE_loss(output, mask)\n",
        "    # dice_loss_value=dice_loss(output, mask)\n",
        "    # loss=0.5*bce_loss+0.5*dice_loss_value\n",
        "    loss=criterion(output, mask)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss+=loss.item()\n",
        "\n",
        "  avg_train_loss = train_loss / len(train_loader)\n",
        "# validation\n",
        "  model.eval()\n",
        "  tatal_loss=0\n",
        "  total_dice_loss=0\n",
        "  total_bce_loss=0\n",
        "  total_dice_score=0\n",
        "  total_iou_score=0\n",
        "  total_val_loss=0\n",
        "  with torch.no_grad():\n",
        "    for images, mask in val_loader:\n",
        "\n",
        "      images=images.to(DEVICE)\n",
        "      mask=mask.to(DEVICE)\n",
        "      output=model(images)\n",
        "      # bce = BCE_loss(output, mask)\n",
        "      # dice_val_loss = dice_loss(output, mask)\n",
        "      # loss = 0.5 * bce + 0.5 * dice_val_loss\n",
        "      loss=criterion(output, mask)\n",
        "      total_val_loss += loss.item()\n",
        "      probs=torch.sigmoid(output)\n",
        "      binary_preds = (probs > 0.5).float()\n",
        "      dice_score, iou_score = calculate_dice_iou(binary_preds, mask)\n",
        "      total_dice_score += dice_score\n",
        "      total_iou_score += iou_score\n",
        "\n",
        "    # Calculate average metrics for the epoch\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    avg_val_dice = total_dice_score / len(val_loader)\n",
        "    avg_val_iou = total_iou_score / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}.. \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}.. \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}.. \"\n",
        "          f\"Val Dice: {avg_val_dice:.4f}.. \"\n",
        "          f\"Val IoU: {avg_val_iou:.4f}\")\n",
        "\n",
        "    # --- Save the best model ---\n",
        "    if 'best_val_dice' in locals() and avg_val_dice > best_val_dice:\n",
        "        best_val_dice = avg_val_dice\n",
        "        save_path = \"/content/drive/MyDrive/RBE595/p3/dataset/model_size_640.pth\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"New best model saved to {save_path} with Dice: {avg_val_dice:.4f}\")"
      ],
      "metadata": {
        "id": "MKRCM4PSbOF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aafd7240-b813-4007-cc79-9a50c2d2e301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10.. Train Loss: 0.1214.. Val Loss: 0.0642.. Val Dice: 0.9204.. Val IoU: 0.8541\n",
            "New best model saved to /content/drive/MyDrive/RBE595/p3/dataset/model_size_640.pth with Dice: 0.9204\n",
            "Epoch 2/10.. Train Loss: 0.0700.. Val Loss: 0.0284.. Val Dice: 0.9693.. Val IoU: 0.9411\n",
            "New best model saved to /content/drive/MyDrive/RBE595/p3/dataset/model_size_640.pth with Dice: 0.9693\n",
            "Epoch 3/10.. Train Loss: 0.0559.. Val Loss: 0.0262.. Val Dice: 0.9732.. Val IoU: 0.9485\n",
            "New best model saved to /content/drive/MyDrive/RBE595/p3/dataset/model_size_640.pth with Dice: 0.9732\n",
            "Epoch 4/10.. Train Loss: 0.0504.. Val Loss: 0.0258.. Val Dice: 0.9715.. Val IoU: 0.9451\n",
            "Epoch 5/10.. Train Loss: 0.0455.. Val Loss: 0.0245.. Val Dice: 0.9730.. Val IoU: 0.9479\n",
            "Epoch 6/10.. Train Loss: 0.0426.. Val Loss: 0.0266.. Val Dice: 0.9701.. Val IoU: 0.9425\n",
            "Epoch 7/10.. Train Loss: 0.0404.. Val Loss: 0.0233.. Val Dice: 0.9752.. Val IoU: 0.9521\n",
            "New best model saved to /content/drive/MyDrive/RBE595/p3/dataset/model_size_640.pth with Dice: 0.9752\n",
            "Epoch 8/10.. Train Loss: 0.0389.. Val Loss: 0.0233.. Val Dice: 0.9747.. Val IoU: 0.9511\n",
            "Epoch 9/10.. Train Loss: 0.0375.. Val Loss: 0.0231.. Val Dice: 0.9741.. Val IoU: 0.9499\n",
            "Epoch 10/10.. Train Loss: 0.0369.. Val Loss: 0.0212.. Val Dice: 0.9772.. Val IoU: 0.9558\n",
            "New best model saved to /content/drive/MyDrive/RBE595/p3/dataset/model_size_640.pth with Dice: 0.9772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Testing ---\n",
        "print(\"\\nTraining Finished. Loading best model for testing.\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/RBE595/p3/dataset/model_12000.pth')) # Load best weights\n",
        "model.eval()\n",
        "\n",
        "total_test_dice = 0\n",
        "total_test_iou = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, mask in test_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        mask = mask.to(DEVICE)\n",
        "        output = model(images)\n",
        "\n",
        "        probs = torch.sigmoid(output)\n",
        "        binary_preds = (probs > 0.5).float()\n",
        "\n",
        "        dice_score, iou_score = calculate_dice_iou(binary_preds, mask)\n",
        "        total_test_dice += dice_score\n",
        "        total_test_iou += iou_score\n",
        "\n",
        "avg_test_dice = total_test_dice / len(test_loader)\n",
        "avg_test_iou = total_test_iou / len(test_loader)\n",
        "\n",
        "print(f\"--- Final Test Metrics ---\")\n",
        "print(f\"Test Dice Score: {avg_test_dice:.4f}\")\n",
        "print(f\"Test IoU Score:  {avg_test_iou:.4f}\")"
      ],
      "metadata": {
        "id": "a5k8m676PRAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7bc108f-901f-4f0c-ea31-64f722663a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Finished. Loading best model for testing.\n",
            "--- Final Test Metrics ---\n",
            "Test Dice Score: 0.9428\n",
            "Test IoU Score:  0.8928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import os\n",
        "# import torchvision.utils as vutils # Import for saving image grids\n",
        "\n",
        "# # --- Testing ---\n",
        "# print(\"\\nTraining Finished. Loading best model for testing.\")\n",
        "# model.load_state_dict(torch.load('UNet_background_attention_1.pth')) # Load best weights\n",
        "# model.eval()\n",
        "\n",
        "# total_test_dice = 0\n",
        "# total_test_iou = 0\n",
        "\n",
        "# # --- Additions for saving output ---\n",
        "# output_dir = \"test_outputs\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "# print(f\"Saving output masks to {output_dir}/\")\n",
        "# # -------------------------------------\n",
        "\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     # Add 'i' for enumeration to get unique filenames\n",
        "#     for i, (images, mask) in enumerate(test_loader):\n",
        "#         images = images.to(DEVICE)\n",
        "#         mask = mask.to(DEVICE)\n",
        "#         output = model(images)\n",
        "\n",
        "#         probs = torch.sigmoid(output)\n",
        "#         binary_preds = (probs > 0.5).float()\n",
        "\n",
        "#         dice_score, iou_score = calculate_dice_iou(binary_preds, mask)\n",
        "#         total_test_dice += dice_score\n",
        "#         total_test_iou += iou_score\n",
        "\n",
        "#         # --- Save the output masks ---\n",
        "#         # This saves the input, ground truth, and prediction for this batch\n",
        "#         # 'vutils.save_image' automatically creates a grid if your batch size > 1\n",
        "\n",
        "#         # Save the original input images\n",
        "#         vutils.save_image(images,\n",
        "#                           os.path.join(output_dir, f\"batch_{i:03d}_input.png\"),\n",
        "#                           normalize=True) # Normalize for proper viewing\n",
        "\n",
        "#         # Save the ground truth masks\n",
        "#         vutils.save_image(mask,\n",
        "#                           os.path.join(output_dir, f\"batch_{i:03d}_ground_truth.png\"))\n",
        "\n",
        "#         # Save your model's predictions\n",
        "#         vutils.save_image(binary_preds,\n",
        "#                           os.path.join(output_dir, f\"batch_{i:03d}_prediction.png\"))\n",
        "#         # -----------------------------\n",
        "\n",
        "# avg_test_dice = total_test_dice / len(test_loader)\n",
        "# avg_test_iou = total_test_iou / len(test_loader)\n",
        "\n",
        "# print(f\"--- Final Test Metrics ---\")\n",
        "# print(f\"Test Dice Score: {avg_test_dice:.4f}\")\n",
        "# print(f\"Test IoU Score:  {avg_test_iou:.4f}\")\n",
        "# Aerial_P3_Unet_2.ipynb_\n",
        "# [1]\n",
        "# 11s\n",
        "\n",
        "# # ┌───────────────────────────────────────────────┐\n",
        "# # │                    IMPORTS                    │\n",
        "# # └───────────────────────────────────────────────┘\n",
        "# import matplotlib.pyplot as plt\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from torchvision import datasets, transforms\n",
        "# from torchvision.transforms import v2\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.optim import AdamW\n",
        "# import zipfile\n",
        "# from PIL import Image, ImageOps\n",
        "# import albumentations as A\n",
        "# from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# import os\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# from torchvision.transforms import v2 as transforms\n",
        "# from PIL import Image\n",
        "# # Determine Device for the whole session\n",
        "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Using device: cuda\n",
        "\n",
        "# [2]\n",
        "# 26s\n",
        "\n",
        "# Mounted at /content/drive\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# # Unzip dataset\n",
        "# file_path=\"/content/drive/MyDrive/RBE595/p3/dataset/window_dataset_12000.zip\"\n",
        "# directory_to_extract_to = \"/content/dataset\" # Define the extraction directory\n",
        "# with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(directory_to_extract_to)\n",
        "\n",
        "# [3]\n",
        "# 0s\n",
        "\n",
        "# image_path = \"/content/dataset/window_dataset/images/\"\n",
        "# mask_path = \"/content/dataset/window_dataset/masks/\"\n",
        "\n",
        "# # List files in the directories to pick one example\n",
        "# import os\n",
        "# image_files = sorted(os.listdir(image_path))\n",
        "# print(len(image_files))\n",
        "# mask_files = sorted(os.listdir(mask_path))\n",
        "\n",
        "# Next steps:\n",
        "# [ ]\n",
        "# 0s\n",
        "# [ ]\n",
        "\n",
        "# # ┌───────────────────────────────────────────────┐\n",
        "# # │                 HYPERPARAMETERS               │\n",
        "# # └───────────────────────────────────────────────┘\n",
        "# NUM_EPOCHS = 10\n",
        "# BATCH_SIZE = 8 # Reduced batch size to potentially resolve OOM error\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# class CustomDataset(Dataset):\n",
        "#     '''\n",
        "#     This dataset loads an image and its corresponding segmentation mask\n",
        "#     using Albumentations for robust augmentation.\n",
        "#     '''\n",
        "#     def __init__(self, image_dir, mask_dir, is_train=True):\n",
        "#         self.image_dir = image_dir\n",
        "#         self.mask_dir = mask_dir\n",
        "\n",
        "#         self.image_files = sorted(os.listdir(self.image_dir))\n",
        "#         self.mask_files = sorted(os.listdir(self.mask_dir))\n",
        "\n",
        "#         if is_train:\n",
        "#             # 1. Training transforms (Merged List)\n",
        "#             self.transform = A.Compose([\n",
        "#                 # --- Essential (Keep at top) ---\n",
        "#                 A.Resize(height=320, width=320, p=1.0),\n",
        "\n",
        "#                 # --- Spatial Augmentations ---\n",
        "#                 A.HorizontalFlip(p=0.5),\n",
        "#                 A.VerticalFlip(p=0.5),      # Updated p=0.5\n",
        "#                 A.Rotate(limit=45, p=0.5),    # ADDED\n",
        "\n",
        "#                 # --- Color Augmentations ---\n",
        "#                 A.ToGray(p=0.1), # Kept from previous version\n",
        "#                 A.RandomBrightnessContrast(p=0.3), # Updated\n",
        "\n",
        "#                 # --- Noise, Blur, and Edge Augmentations ---\n",
        "#                 A.GaussNoise(p=0.4),               # Updated\n",
        "#                 A.GaussianBlur(blur_limit=(3, 7), p=0.4), # ADDED\n",
        "#                 A.MotionBlur(blur_limit=7, p=0.3),      # ADDED\n",
        "#                 A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.4), # ADDED\n",
        "\n",
        "#                 # --- Dropout Augmentation ---\n",
        "#                 A.CoarseDropout(\n",
        "#                     max_holes=8, max_height=20, max_width=20, # Updated\n",
        "#                     min_holes=3, min_height=5, min_width=5,\n",
        "#                     fill_value=0, p=0.4\n",
        "#                 ),\n",
        "\n",
        "#                 # --- Final conversion (Keep at bottom) ---\n",
        "#                 A.Normalize(\n",
        "#                     mean=[0.485, 0.456, 0.406],\n",
        "#                     std=[0.229, 0.224, 0.225]\n",
        "#                 ),\n",
        "#                 ToTensorV2() # Converts image and mask to PyTorch tensors\n",
        "#             ])\n",
        "#         else:\n",
        "#             # 2. Validation/Test transforms (Unchanged)\n",
        "#             self.transform = A.Compose([\n",
        "#                 A.Resize(height=320, width=320, p=1.0),\n",
        "#                 A.Normalize(\n",
        "#                     mean=[0.485, 0.456, 0.406],\n",
        "#                     std=[0.229, 0.224, 0.225]\n",
        "#                 ),\n",
        "#                 ToTensorV2()\n",
        "#             ])\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.image_files)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_name = self.image_files[idx]\n",
        "#         mask_name = self.mask_files[idx]\n",
        "\n",
        "#         img_path = os.path.join(self.image_dir, img_name)\n",
        "#         mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "#         image = Image.open(img_path).convert(\"RGB\")\n",
        "#         image = ImageOps.exif_transpose(image) # Auto-orientation\n",
        "\n",
        "#         mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "#         image_np = np.array(image)\n",
        "#         mask_np = np.array(mask)\n",
        "\n",
        "#         # Ensure mask is binary (0 or 1)\n",
        "#         mask_np = (mask_np > 128).astype(np.uint8) * 255 # Assuming mask is grayscale with values > 128 as foreground\n",
        "\n",
        "#         transformed = self.transform(image=image_np, mask=mask_np)\n",
        "\n",
        "#         image_tensor = transformed['image']\n",
        "#         mask_tensor = transformed['mask'].unsqueeze(0).float() / 255.0 # Normalize mask to 0-1\n",
        "\n",
        "#         return image_tensor, mask_tensor\n",
        "\n",
        "# # --- Create Train, Validation, and Test DataLoaders ---\n",
        "\n",
        "# # Define your paths and parameters\n",
        "# image_path = \"/content/dataset/window_dataset/images/\"\n",
        "# mask_path = \"/content/dataset/window_dataset/masks/\"\n",
        "\n",
        "\n",
        "# # Define split ratios\n",
        "# TRAIN_RATIO = 0.8  # 80% for training\n",
        "# VAL_RATIO = 0.1   # 10% for validation\n",
        "# TEST_RATIO = 0.1  # 10% for testing\n",
        "\n",
        "# # Create the full dataset (with training augmentation for now)\n",
        "# full_dataset = CustomDataset(image_path, mask_path, is_train=True)\n",
        "\n",
        "# # Calculate split sizes\n",
        "# dataset_size = len(full_dataset)\n",
        "# train_size = int(TRAIN_RATIO * dataset_size)\n",
        "# val_size = int(VAL_RATIO * dataset_size)\n",
        "# test_size = dataset_size - train_size - val_size  # Remaining goes to test\n",
        "\n",
        "# print(f\"Total dataset size: {dataset_size}\")\n",
        "# print(f\"Train size: {train_size}\")\n",
        "# print(f\"Validation size: {val_size}\")\n",
        "# print(f\"Test size: {test_size}\")\n",
        "\n",
        "# # Split the dataset\n",
        "# train_dataset, val_dataset, test_dataset = random_split(\n",
        "#     full_dataset,\n",
        "#     [train_size, val_size, test_size],\n",
        "#     generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
        "# )\n",
        "\n",
        "# # Update the is_train flag for validation and test datasets\n",
        "# # Create new dataset instances for val and test without augmentation\n",
        "# val_dataset_no_aug = CustomDataset(image_path, mask_path, is_train=False)\n",
        "# test_dataset_no_aug = CustomDataset(image_path, mask_path, is_train=False)\n",
        "\n",
        "# # Get the same indices from the split\n",
        "# val_indices = val_dataset.indices\n",
        "# test_indices = test_dataset.indices\n",
        "\n",
        "# # Create subsets with no augmentation\n",
        "# from torch.utils.data import Subset\n",
        "# val_dataset = Subset(val_dataset_no_aug, val_indices)\n",
        "# test_dataset = Subset(test_dataset_no_aug, test_indices)\n",
        "\n",
        "# # Create DataLoaders\n",
        "# train_loader = DataLoader(\n",
        "#     train_dataset,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle=True,  # Shuffle training data\n",
        "#     num_workers=2,  # Parallel data loading\n",
        "#     pin_memory=True  # Faster data transfer to GPU\n",
        "# )\n",
        "\n",
        "# val_loader = DataLoader(\n",
        "#     val_dataset,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle=False,  # Don't shuffle validation data\n",
        "#     num_workers=2,\n",
        "#     pin_memory=True\n",
        "# )\n",
        "\n",
        "# test_loader = DataLoader(\n",
        "#     test_dataset,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle=False,  # Don't shuffle test data\n",
        "#     num_workers=2,\n",
        "#     pin_memory=True\n",
        "# )\n",
        "\n",
        "# # Print DataLoader information\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "# print(\"DATALOADER INFORMATION\")\n",
        "# print(\"=\"*50)\n",
        "# print(f\"\\nTrain loader: {len(train_loader)} batches\")\n",
        "# print(f\"Validation loader: {len(val_loader)} batches\")\n",
        "# print(f\"Test loader: {len(test_loader)} batches\")\n",
        "\n",
        "# # Verify shapes\n",
        "# try:\n",
        "#     train_images, train_masks = next(iter(train_loader))\n",
        "#     print(f\"\\nTrain batch - Images shape: {train_images.shape}, Masks shape: {train_masks.shape}\")\n",
        "\n",
        "#     val_images, val_masks = next(iter(val_loader))\n",
        "#     print(f\"Val batch - Images shape: {val_images.shape}, Masks shape: {val_masks.shape}\")\n",
        "\n",
        "#     test_images, test_masks = next(iter(test_loader))\n",
        "#     print(f\"Test batch - Images shape: {test_images.shape}, Masks shape: {test_masks.shape}\")\n",
        "\n",
        "#     print(\"\\n✓ All DataLoaders created successfully!\")\n",
        "#     print(\"=\"*50)\n",
        "# except Exception as e:\n",
        "#     print(f\"\\nError loading batches: {e}\")\n",
        "\n",
        "# Total dataset size: 7000\n",
        "# Train size: 5600\n",
        "# Validation size: 700\n",
        "# Test size: 700\n",
        "\n",
        "# ==================================================\n",
        "# DATALOADER INFORMATION\n",
        "# ==================================================\n",
        "\n",
        "# Train loader: 700 batches\n",
        "# Validation loader: 88 batches\n",
        "# Test loader: 88 batches\n",
        "\n",
        "# /tmp/ipython-input-2431407332.py:35: UserWarning: Argument(s) 'max_holes, max_height, max_width, min_holes, min_height, min_width, fill_value' are not valid for transform CoarseDropout\n",
        "#   A.CoarseDropout(\n",
        "\n",
        "\n",
        "# Train batch - Images shape: torch.Size([8, 3, 320, 320]), Masks shape: torch.Size([8, 1, 320, 320])\n",
        "# Val batch - Images shape: torch.Size([8, 3, 320, 320]), Masks shape: torch.Size([8, 1, 320, 320])\n",
        "# Test batch - Images shape: torch.Size([8, 3, 320, 320]), Masks shape: torch.Size([8, 1, 320, 320])\n",
        "\n",
        "# ✓ All DataLoaders created successfully!\n",
        "# ==================================================\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # --- Attention Block (as you provided) ---\n",
        "# class AttentionBlock(nn.Module):\n",
        "#     def __init__(self, F_g, F_l, F_int):\n",
        "#         super(AttentionBlock, self).__init__()\n",
        "#         self.W_g = nn.Sequential(\n",
        "#             nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "#             nn.BatchNorm2d(F_int)\n",
        "#         )\n",
        "#         self.W_x = nn.Sequential(\n",
        "#             nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "#             nn.BatchNorm2d(F_int)\n",
        "#         )\n",
        "#         self.psi = nn.Sequential(\n",
        "#             nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "#             nn.BatchNorm2d(1),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "#     def forward(self, g, x):\n",
        "#         g1 = self.W_g(g)\n",
        "#         x1 = self.W_x(x)\n",
        "#         psi = self.relu(g1 + x1)\n",
        "#         psi = self.psi(psi)\n",
        "#         return x * psi\n",
        "\n",
        "# # --- Double Conv Block (as you provided) ---\n",
        "# def double_conv(in_channels, out_channels):\n",
        "#     '''\n",
        "#     Return a Convolutional Block with Two Convolutional Layers with ReLU Activation based on in_channels and\n",
        "#     out_channels, with a stride of 3 and padding of 1. If input has H and W, then output will have same H and W.\n",
        "#     '''\n",
        "#     return nn.Sequential(\n",
        "#         nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True),\n",
        "#         nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     )\n",
        "\n",
        "# # --- NEW: Attention U-Net Class (Modified) ---\n",
        "# class UNet(nn.Module):\n",
        "#     '''\n",
        "#     Implementation of an Attention U-Net (3-level).\n",
        "#     '''\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # --- Encoder (3 levels) ---\n",
        "#         self.dconv_down1 = double_conv(in_channels, 64)\n",
        "#         self.dconv_down2 = double_conv(64, 128)\n",
        "#         self.dconv_down3 = double_conv(128, 256)\n",
        "#         # Removed dconv_down4\n",
        "#         self.maxpool = nn.MaxPool2d(2)\n",
        "\n",
        "#         # --- Bottleneck ---\n",
        "#         # Input from 3rd level (256 channels), output 512\n",
        "#         self.bottleneck = double_conv(256, 512)\n",
        "\n",
        "#         # --- Decoder ---\n",
        "#         self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "#         # --- Attention Blocks ---\n",
        "#         # Removed att4\n",
        "#         # F_g=512 (from upsample(bottleneck)), F_l=256 (from conv3)\n",
        "#         self.att3 = AttentionBlock(F_g=512, F_l=256, F_int=128)\n",
        "#         # F_g=256 (from dconv_up3), F_l=128 (from conv2)\n",
        "#         self.att2 = AttentionBlock(F_g=256, F_l=128, F_int=64)\n",
        "#         # F_g=128 (from dconv_up2), F_l=64 (from conv1)\n",
        "#         self.att1 = AttentionBlock(F_g=128, F_l=64, F_int=32)\n",
        "\n",
        "#         # --- Decoder Convolutions ---\n",
        "#         # Removed dconv_up4\n",
        "#         # Input: (g=512) + (att3=256) = 768\n",
        "#         self.dconv_up3 = double_conv(512 + 256, 256)  # 768  -> 256\n",
        "#         # Input: (g=256) + (att2=128) = 384\n",
        "#         self.dconv_up2 = double_conv(256 + 128, 128)  # 384  -> 128\n",
        "#         # Input: (g=128) + (att1=64) = 192\n",
        "#         self.dconv_up1 = double_conv(128 + 64, 64)   # 192  -> 64\n",
        "\n",
        "#         # --- Output Layer ---\n",
        "#         self.conv_last = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         '''\n",
        "#         Forward Pass Method for the Attention UNet\n",
        "#         '''\n",
        "\n",
        "#         # ┌───────────────────────────────────────────────┐\n",
        "#         # │         ENCODER BLOCKS (DOWNSAMPLING)         │\n",
        "#         # └───────────────────────────────────────────────┘\n",
        "#         conv1 = self.dconv_down1(x)                                 # (N, 64, H, W)\n",
        "#         x = self.maxpool(conv1)\n",
        "\n",
        "#         conv2 = self.dconv_down2(x)                                 # (N, 128, H/2, W/2)\n",
        "#         x = self.maxpool(conv2)\n",
        "\n",
        "#         conv3 = self.dconv_down3(x)                                 # (N, 256, H/4, W/4)\n",
        "#         x = self.maxpool(conv3)                                     # (N, 256, H/8, W/8)\n",
        "\n",
        "#         # Removed 4th Encoder Block\n",
        "\n",
        "#         # ┌───────────────────────────────────────────────┐\n",
        "#         # │         BOTTLENECK                            │\n",
        "#         # └───────────────────────────────────────────────┘\n",
        "#         x = self.bottleneck(x)                                      # (N, 512, H/8, W/8)\n",
        "\n",
        "#         # ┌───────────────────────────────────────────────┐\n",
        "#         # │           DECODER BLOCKS (UPSAMPLING)         │\n",
        "#         # └───────────────────────────────────────────────┘\n",
        "\n",
        "#         # --- First Decoder Block (was Second) ---\n",
        "#         g = self.upsample(x)                                        # (N, 512, H/4, W/4)\n",
        "#         x_skip3 = conv3                                             # (N, 256, H/4, W/4)\n",
        "#         att3 = self.att3(g=g, x=x_skip3)                            # (N, 256, H/4, W/4)\n",
        "#         x = torch.cat([g, att3], dim=1)                             # (N, 512 + 256, H/4, W/4)\n",
        "#         x = self.dconv_up3(x)                                       # (N, 256, H/4, W/4)\n",
        "\n",
        "#         # --- Second Decoder Block (was Third) ---\n",
        "#         g = self.upsample(x)                                        # (N, 256, H/2, W/2)\n",
        "#         x_skip2 = conv2                                             # (N, 128, H/2, W/2)\n",
        "#         att2 = self.att2(g=g, x=x_skip2)                            # (N, 128, H/2, W/2)\n",
        "#         x = torch.cat([g, att2], dim=1)                             # (N, 256 + 128, H/2, W/2)\n",
        "#         x = self.dconv_up2(x)                                       # (N, 128, H/2, W/2)\n",
        "\n",
        "#         # --- Third Decoder Block (was Fourth) ---\n",
        "#         g = self.upsample(x)                                        # (N, 128, H, W)\n",
        "#         x_skip1 = conv1                                             # (N, 64, H, W)\n",
        "#         att1 = self.att1(g=g, x=x_skip1)                            # (N, 64, H, W)\n",
        "#         x = torch.cat([g, att1], dim=1)                             # (N, 128 + 64, H, W)\n",
        "#         x = self.dconv_up1(x)                                       # (N, 64, H, W)\n",
        "\n",
        "#         # --- Final Output Layer ---\n",
        "#         out = self.conv_last(x)                                     # (N, out_channels, H, W)\n",
        "\n",
        "#         return out\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# class CombinedLoss(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Combination of BCE, Dice, and boundary loss for clean edges\n",
        "#     \"\"\"\n",
        "#     def __init__(self, bce_weight=0.5, dice_weight=0.3, boundary_weight=0.2):\n",
        "#         super().__init__()\n",
        "#         self.bce_weight = bce_weight\n",
        "#         self.dice_weight = dice_weight\n",
        "#         self.boundary_weight = boundary_weight\n",
        "#         self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#     def dice_loss(self, pred, target, smooth=1e-6):\n",
        "#         pred = torch.sigmoid(pred)\n",
        "#         intersection = (pred * target).sum(dim=(2, 3))\n",
        "#         union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
        "#         dice = (2. * intersection + smooth) / (union + smooth)\n",
        "#         return 1 - dice.mean()\n",
        "\n",
        "#     def boundary_loss(self, pred, target):\n",
        "#         \"\"\"\n",
        "#         Penalize predictions at boundaries more heavily\n",
        "#         \"\"\"\n",
        "#         pred = torch.sigmoid(pred)\n",
        "\n",
        "#         # Compute gradient of target (boundaries)\n",
        "#         kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
        "#                                 dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n",
        "#         kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
        "#                                 dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n",
        "\n",
        "#         # Detect edges in target\n",
        "#         edge_x = F.conv2d(target, kernel_x, padding=1)\n",
        "#         edge_y = F.conv2d(target, kernel_y, padding=1)\n",
        "#         edges = torch.sqrt(edge_x**2 + edge_y**2)\n",
        "#         edges = (edges > 0.5).float()\n",
        "\n",
        "#         # Weight loss by edges (penalize errors at boundaries more)\n",
        "#         boundary_error = F.binary_cross_entropy(pred, target, reduction='none')\n",
        "#         boundary_error = boundary_error * (1 + 5 * edges)  # 5x penalty at edges\n",
        "\n",
        "#         return boundary_error.mean()\n",
        "\n",
        "#     def forward(self, pred, target):\n",
        "#         bce = self.bce(pred, target)\n",
        "#         dice = self.dice_loss(pred, target)\n",
        "#         boundary = self.boundary_loss(pred, target)\n",
        "\n",
        "#         total = (self.bce_weight * bce +\n",
        "#                 self.dice_weight * dice +\n",
        "#                 self.boundary_weight * boundary)\n",
        "#         return total\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# # Model initialisation\n",
        "\n",
        "# model= UNet(in_channels=3, out_channels=1).to(DEVICE)\n",
        "# BCE_loss = nn.BCEWithLogitsLoss()\n",
        "# criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.3, boundary_weight=0.2)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "# def calculate_dice_iou(inputs, targets, smooth=1e-7):\n",
        "#     \"\"\"\n",
        "#     Calculates Dice and IoU scores for a batch.\n",
        "#     Assumes inputs are binary predictions (0.0 or 1.0) and targets are binary masks.\n",
        "#     \"\"\"\n",
        "#     # Flatten tensors\n",
        "#     inputs = inputs.view(-1)\n",
        "#     targets = targets.view(-1)\n",
        "\n",
        "#     # Calculate intersection\n",
        "#     intersection = (inputs * targets).sum()\n",
        "\n",
        "#     # Calculate Dice Score\n",
        "#     total_sum = inputs.sum() + targets.sum()\n",
        "#     dice = (2. * intersection + smooth) / (total_sum + smooth)\n",
        "\n",
        "#     # Calculate IoU (Jaccard Index)\n",
        "#     union = total_sum - intersection\n",
        "#     iou = (intersection + smooth) / (union + smooth)\n",
        "\n",
        "#     return dice.item(), iou.item()\n",
        "\n",
        "# [ ]\n",
        "\n",
        "\n",
        "# # Training\n",
        "# best_val_dice = -1  # Initialize best_val_dice to a low value\n",
        "\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#   train_loss=0\n",
        "\n",
        "#   model.train()\n",
        "#   for images, mask in train_loader:\n",
        "#     images=images.to(DEVICE)\n",
        "#     mask=mask.to(DEVICE)\n",
        "#     optimizer.zero_grad()\n",
        "#     output=model(images)\n",
        "#     # bce_loss=BCE_loss(output, mask)\n",
        "#     # dice_loss_value=dice_loss(output, mask)\n",
        "#     # loss=0.5*bce_loss+0.5*dice_loss_value\n",
        "#     loss=criterion(output, mask)\n",
        "\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     train_loss+=loss.item()\n",
        "\n",
        "#   avg_train_loss = train_loss / len(train_loader)\n",
        "# # validation\n",
        "#   model.eval()\n",
        "#   tatal_loss=0\n",
        "#   total_dice_loss=0\n",
        "#   total_bce_loss=0\n",
        "#   total_dice_score=0\n",
        "#   total_iou_score=0\n",
        "#   total_val_loss=0\n",
        "#   with torch.no_grad():\n",
        "#     for images, mask in val_loader:\n",
        "\n",
        "#       images=images.to(DEVICE)\n",
        "#       mask=mask.to(DEVICE)\n",
        "#       output=model(images)\n",
        "#       # bce = BCE_loss(output, mask)\n",
        "#       # dice_val_loss = dice_loss(output, mask)\n",
        "#       # loss = 0.5 * bce + 0.5 * dice_val_loss\n",
        "#       loss=criterion(output, mask)\n",
        "#       total_val_loss += loss.item()\n",
        "#       probs=torch.sigmoid(output)\n",
        "#       binary_preds = (probs > 0.5).float()\n",
        "#       dice_score, iou_score = calculate_dice_iou(binary_preds, mask)\n",
        "#       total_dice_score += dice_score\n",
        "#       total_iou_score += iou_score\n",
        "\n",
        "#     # Calculate average metrics for the epoch\n",
        "#     avg_val_loss = total_val_loss / len(val_loader)\n",
        "#     avg_val_dice = total_dice_score / len(val_loader)\n",
        "#     avg_val_iou = total_iou_score / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{NUM_EPOCHS}.. \"\n",
        "#           f\"Train Loss: {avg_train_loss:.4f}.. \"\n",
        "#           f\"Val Loss: {avg_val_loss:.4f}.. \"\n",
        "#           f\"Val Dice: {avg_val_dice:.4f}.. \"\n",
        "#           f\"Val IoU: {avg_val_iou:.4f}\")\n",
        "\n",
        "#     # --- Save the best model ---\n",
        "#     if 'best_val_dice' in locals() and avg_val_dice > best_val_dice:\n",
        "#         best_val_dice = avg_val_dice\n",
        "#         save_path = \"/content/drive/MyDrive/RBE595/p3/dataset/model_12000.pth\"\n",
        "#         torch.save(model.state_dict(), save_path)\n",
        "#         print(f\"New best model saved to {save_path} with Dice: {avg_val_dice:.4f}\")\n",
        "\n",
        "# Epoch 1/10.. Train Loss: 0.1810.. Val Loss: 0.0342.. Val Dice: 0.9704.. Val IoU: 0.9427\n",
        "# New best model saved with Dice: 0.9704\n",
        "# Epoch 2/10.. Train Loss: 0.1020.. Val Loss: 0.0277.. Val Dice: 0.9736.. Val IoU: 0.9487\n",
        "# New best model saved with Dice: 0.9736\n",
        "# Epoch 3/10.. Train Loss: 0.0860.. Val Loss: 0.0306.. Val Dice: 0.9674.. Val IoU: 0.9372\n",
        "# Epoch 4/10.. Train Loss: 0.0833.. Val Loss: 0.0214.. Val Dice: 0.9782.. Val IoU: 0.9574\n",
        "# New best model saved with Dice: 0.9782\n",
        "# Epoch 5/10.. Train Loss: 0.0747.. Val Loss: 0.0210.. Val Dice: 0.9785.. Val IoU: 0.9580\n",
        "# New best model saved with Dice: 0.9785\n",
        "# Epoch 6/10.. Train Loss: 0.0692.. Val Loss: 0.0162.. Val Dice: 0.9831.. Val IoU: 0.9668\n",
        "# New best model saved with Dice: 0.9831\n",
        "# Epoch 7/10.. Train Loss: 0.0637.. Val Loss: 0.0181.. Val Dice: 0.9805.. Val IoU: 0.9618\n",
        "# Epoch 8/10.. Train Loss: 0.0586.. Val Loss: 0.0169.. Val Dice: 0.9818.. Val IoU: 0.9643\n",
        "# Epoch 9/10.. Train Loss: 0.0553.. Val Loss: 0.0233.. Val Dice: 0.9718.. Val IoU: 0.9454\n",
        "# Epoch 10/10.. Train Loss: 0.0552.. Val Loss: 0.0166.. Val Dice: 0.9829.. Val IoU: 0.9664\n",
        "\n",
        "# [ ]\n",
        "\n",
        "# # --- Testing ---\n",
        "# print(\"\\nTraining Finished. Loading best model for testing.\")\n",
        "# model.load_state_dict(torch.load('UNet_background_attention_1.pth')) # Load best weights\n",
        "# model.eval()\n",
        "\n",
        "# total_test_dice = 0\n",
        "# total_test_iou = 0\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for images, mask in test_loader:\n",
        "#         images = images.to(DEVICE)\n",
        "#         mask = mask.to(DEVICE)\n",
        "#         output = model(images)\n",
        "\n",
        "#         probs = torch.sigmoid(output)\n",
        "#         binary_preds = (probs > 0.5).float()\n",
        "\n",
        "#         dice_score, iou_score = calculate_dice_iou(binary_preds, mask)\n",
        "#         total_test_dice += dice_score\n",
        "#         total_test_iou += iou_score\n",
        "\n",
        "# avg_test_dice = total_test_dice / len(test_loader)\n",
        "# avg_test_iou = total_test_iou / len(test_loader)\n",
        "\n",
        "# print(f\"--- Final Test Metrics ---\")\n",
        "# print(f\"Test Dice Score: {avg_test_dice:.4f}\")\n",
        "# print(f\"Test IoU Score:  {avg_test_iou:.4f}\")\n",
        "\n",
        "\n",
        "# Training Finished. Loading best model for testing.\n",
        "# --- Final Test Metrics ---\n",
        "# Test Dice Score: 0.9829\n",
        "# Test IoU Score:  0.9665\n",
        "\n",
        "# [ ]\n",
        "\n",
        "\n",
        "# Training Finished. Loading best model for testing.\n",
        "# Saving output masks to test_outputs/\n",
        "# --- Final Test Metrics ---\n",
        "# Test Dice Score: 0.9829\n",
        "# Test IoU Score:  0.9665\n",
        "\n",
        "# Finished testing. All output images saved to 'test_outputs'.\n",
        "\n",
        "# [ ]\n",
        "# Colab paid products - Cancel contracts here\n",
        "\n",
        "\n",
        "# print(f\"\\nFinished testing. All output images saved to '{output_dir}'.\")"
      ],
      "metadata": {
        "id": "yD3pTBUVYBzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDeEGq6AK8A_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}